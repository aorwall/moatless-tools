import concurrent.futures
import gc
import json
import logging
import os
from pathlib import Path
import shutil
import time
import traceback
from datetime import datetime, timezone
from typing import Any, Union, Callable, List, Dict, Tuple

import filelock

from moatless.runtime.testbed import TestbedEnvironment
from moatless.benchmark.report import to_result
from moatless.events import BaseEvent, event_bus
from moatless.config.model_config import create_completion_model
from moatless.config.agent_config import get_agent
from moatless.flow import AgenticSystem, FlowCompletedEvent, FlowErrorEvent
from moatless.benchmark.schema import (
    TreeSearchSettings,
    Evaluation,
    EvaluationInstance,
    EvaluationStatus,
    InstanceStatus,
    EvaluationEvent,
)
from moatless.benchmark.swebench import (
    create_repository,
    create_index,
)
from moatless.benchmark.swebench.utils import create_repository_async, instance_repo_path
from moatless.benchmark.utils import get_moatless_instance, load_moatless_datasets
from moatless.completion import BaseCompletionModel
from moatless.events import SystemEvent
from moatless.file_context import FileContext
from moatless.flow.loop import AgenticLoop
from moatless.node import Node
from moatless.runtime.testbed import TestbedEnvironment
from moatless.search_tree import SearchTree

import asyncio
from asyncio import Task
from moatless.context_data import current_evaluation_name

from moatless.utils.moatless import get_moatless_dir, get_moatless_trajectory_dir
from moatless.workspace import Workspace
from testbeds.sdk.sdk import TestbedSDK

# Custom JSON encoder for datetime objects
class DateTimeEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, datetime):
            return obj.isoformat()
        return super().default(obj)


logger = logging.getLogger(__name__)

__all__ = [
    "TreeSearchSettings",
    "Evaluation",
    "InstanceStatus",
    "EvaluationStatus",
    "EvaluationEvent",
]


class EvaluationRunner:
    def __init__(
        self,
        evaluation: Evaluation,
        repo_base_dir: Union[str, None] = None,
        evaluations_dir: Union[str, None] = None,
        max_concurrent_instances: int = 1,
        remove_repo_after_evaluation: bool = True,
    ):
        self.tree_search_settings = evaluation.settings
        self.evaluation = evaluation
        self.max_concurrent_instances = max_concurrent_instances

        if evaluations_dir:
            self.evaluations_dir = evaluations_dir
        else:
            self.evaluations_dir = get_moatless_dir() / "evals"

        self.evaluation_dir = Path(self.evaluations_dir) / self.evaluation.evaluation_name
        self.locks_dir = self.evaluation_dir / ".locks"
        self.locks_dir.mkdir(exist_ok=True)

        self.repo_base_dir = repo_base_dir or os.getenv("MOATLESS_REPO_DIR", "/tmp/repos")
        #event_bus.subscribe(self._handle_events)

        # Subscribe to flow events
        event_bus.subscribe(self._handle_flow_events)

        # Add state lock
        self._state_lock = asyncio.Lock()
        self._state_changed = asyncio.Event()
        self._tasks: List[asyncio.Task] = []
        self._active_runs: Dict[str, Tuple[AgenticSystem, asyncio.Task]] = {}
        self._last_status_log = 0
        self._status_log_interval = 30  # Log status every 30 seconds if no changes

    async def run_evaluation(self):
        """Start and manage the evaluation process."""
        try:
            current_evaluation_name.set(self.evaluation.evaluation_name)
            
            logger.info(f"Running evaluation: {self.evaluation.evaluation_name}")
            self.evaluation.status = EvaluationStatus.RUNNING
            self.evaluation.started_at = datetime.now(timezone.utc)
            self._save_evaluation(self.evaluation)
            
            await self.emit_event("evaluation_started")

            # Start status monitoring task
            status_monitor = self._create_tracked_task(
                self._monitor_status(),
                "status_monitor"
            )

            # Start the state machine loop
            while True:
                await self._process_instances()
                
                # Check if evaluation is complete
                if await self._check_evaluation_completion():
                    break
                
                try:
                    # Wait for state changes or timeout
                    await asyncio.wait_for(self._state_changed.wait(), timeout=5.0)
                    self._state_changed.clear()
                except asyncio.TimeoutError:
                    # No state changes, continue checking periodically
                    pass

            # Wait for all remaining tasks to complete
            if self._tasks:
                logger.info(f"Waiting for {len(self._tasks)} remaining tasks to complete...")
                await asyncio.gather(*self._tasks, return_exceptions=True)

        except Exception as e:
            logger.exception("Error running evaluation")
            self.evaluation.status = EvaluationStatus.ERROR
            self.evaluation.error = str(e)
            self._save_evaluation(self.evaluation)
            await self.emit_event("evaluation_error", {"error": str(e)})
            raise

    async def _process_instances(self):
        """Process all instances based on their current state."""
        # Get counts of instances in each state
        current_states = await self._get_instance_states()
        counts = {status.value: 0 for status in InstanceStatus}
        for status in current_states.values():
            counts[status] += 1
        
        # Start setups if capacity available
        if counts[InstanceStatus.SETTING_UP] < self.max_concurrent_instances:
            await self._start_new_setups(counts[InstanceStatus.SETTING_UP])

        # Start runs if capacity available  
        if counts[InstanceStatus.RUNNING] < self.max_concurrent_instances:
            await self._start_new_runs(counts[InstanceStatus.RUNNING])

        # Handle evaluations
        async with self._state_lock:
            evaluating_instances = [
                i for i in self.evaluation.instances 
                if i.status == InstanceStatus.EVALUATING
            ]
        
        for instance in evaluating_instances:
            self._create_tracked_task(
                self._evaluate_instance(instance),
                instance.instance_id
            )

    async def _get_instance_states(self) -> dict[str, InstanceStatus]:
        """Get current instance states in a thread-safe way."""
        async with self._state_lock:
            return {
                instance.instance_id: instance.status 
                for instance in self.evaluation.instances
            }

    async def _start_new_setups(self, current_setups: int):
        """Start new instance setups if capacity available."""
        available_slots = self.max_concurrent_instances - current_setups
        if available_slots <= 0:
            return

        # Find instances that need setup
        created_instances = [
            i for i in self.evaluation.instances 
            if i.status == InstanceStatus.CREATED
        ][:available_slots]

        for instance in created_instances:
            logger.info(f"Starting setup for instance {instance.instance_id}")
            await self._update_instance_state(instance, InstanceStatus.SETTING_UP)
            self._create_tracked_task(
                self.setup_instance(instance.instance_id),
                instance.instance_id
            )

    async def _start_new_runs(self, current_runs: int):
        """Start new instance runs if capacity available."""
        available_slots = self.max_concurrent_instances - current_runs
        if available_slots <= 0:
            return

        # Find instances ready to run
        pending_instances = [
            i for i in self.evaluation.instances 
            if i.status == InstanceStatus.PENDING
        ][:available_slots]

        for instance in pending_instances:
            logger.info(f"Starting run for instance {instance.instance_id}")
            await self._update_instance_state(instance, InstanceStatus.RUNNING)
            self._create_tracked_task(
                self.create_and_run_instance(instance.instance_id),
                instance.instance_id
            )

    async def _check_evaluation_completion(self) -> bool:
        """Check if evaluation is complete."""
        if self.evaluation.status != EvaluationStatus.RUNNING:
            return True

        # Count finished instances
        total_instances = len(self.evaluation.instances)
        finished_instances = sum(
            1 for i in self.evaluation.instances 
            if i.status in [InstanceStatus.EVALUATED, InstanceStatus.ERROR]
        )

        if finished_instances == total_instances:
            logger.info(f"All instances finished for evaluation {self.evaluation.evaluation_name}")
            self.evaluation.status = EvaluationStatus.COMPLETED
            self.evaluation.completed_at = datetime.now(timezone.utc)
            self._save_evaluation(self.evaluation)
            
            # Get final counts for the completion event
            counts = await self._get_instance_states()
            await self.emit_event("evaluation_completed", {
                "total_completed": sum(1 for i, status in counts.items() if status == InstanceStatus.EVALUATED),
                "total_errors": sum(1 for i, status in counts.items() if status == InstanceStatus.ERROR)
            })
            return True

        return False

    async def _update_instance_state(self, instance: EvaluationInstance, new_status: InstanceStatus, error: str = None):
        """Update instance state in a thread-safe way."""
        async with self._state_lock:
            old_status = instance.status
            if old_status != new_status:
                instance.status = new_status
                if error:
                    instance.error = error
                logger.info(f"Instance {instance.instance_id} transitioned: {old_status} -> {new_status}")
                self._save_evaluation(self.evaluation)
                self._state_changed.set()
                self._last_status_log = time.time()  # Reset status log timer on state change
                
                # Print status immediately on state change
                counts = await self._get_instance_counts()
                self._print_status_with_counts(counts)

    async def setup_instance(self, instance_id: str):
        """Set up a single instance."""
        instance = self.evaluation.get_instance(instance_id)
        if not instance:
            raise ValueError(f"Instance {instance_id} not found")

        try:
            await self.emit_event("instance_setup_started", {"instance_id": instance_id})
            instance.started_at = datetime.now(timezone.utc)

            moatless_instance = get_moatless_instance(instance_id=instance_id)
            problem_statement = f"<task>\nSolve the following reported issue in the {moatless_instance['repo']} repository:\n\n{moatless_instance['problem_statement']}\n</task>"

            # Create the agentic flow but don't start it yet
            agentic_system = await self.create_agentic_flow(
                problem_statement=problem_statement,
                moatless_instance=moatless_instance,
            )

            # Store the agentic system for later use
            self._store_agentic_system(instance_id, agentic_system)
            
            # Mark as ready to run
            await self._update_instance_state(instance, InstanceStatus.PENDING)
            await self.emit_event("instance_setup_completed", {"instance_id": instance_id})

        except Exception as e:
            logger.exception(f"Error setting up instance {instance_id}")
            await self._update_instance_state(instance, InstanceStatus.ERROR, str(e))
            await self.emit_event("instance_error", {"instance_id": instance_id, "error": str(e)})
            raise

    def _store_agentic_system(self, instance_id: str, system: AgenticSystem):
        """Store the prepared agentic system for an instance."""
        if not hasattr(self, '_agentic_systems'):
            self._agentic_systems = {}
        self._agentic_systems[instance_id] = system

    async def create_and_run_instance(self, instance_id: str):
        """Create and run a single instance."""
        instance = self.evaluation.get_instance(instance_id)
        if not instance:
            raise ValueError(f"Instance {instance_id} not found")

        logger.info(f"Starting evaluation of instance {instance_id}")
        await self._update_instance_state(instance, InstanceStatus.RUNNING)
        await self.emit_event("instance_started", {"instance_id": instance_id})

        try:
            # Get the prepared agentic system
            agentic_system = self._agentic_systems.get(instance_id)
            if not agentic_system:
                raise ValueError(f"No prepared agentic system found for instance {instance_id}")

            await self._start_agentic_system(agentic_system)
            logger.info(f"Started agentic system for instance {instance_id}")
            
        except Exception as e:
            logger.exception(f"Error running instance {instance_id}")
            await self._update_instance_state(instance, InstanceStatus.ERROR, str(e))
            await self.emit_event("instance_error", {"instance_id": instance_id, "error": str(e)})
            raise

    async def create_agentic_flow(
        self,
        problem_statement: str,
        moatless_instance: dict,
    ) -> AgenticSystem:
    
        """Create an agentic system for the instance."""
        repository = await create_repository_async(moatless_instance, repo_base_dir=self.repo_base_dir)
        code_index = create_index(moatless_instance, repository=repository)

        instance_id = moatless_instance["instance_id"]
        
        completion_model = create_completion_model(self.tree_search_settings.model_id)
        completion_model.metadata = {"instance_id": instance_id}

        runtime = None
        testbed_log_dir = os.path.join(get_moatless_trajectory_dir(instance_id, self.evaluation.evaluation_name), "testbed_logs")
        if not os.path.exists(testbed_log_dir):
            os.makedirs(testbed_log_dir)

        runtime = TestbedEnvironment(
            repository=repository,
            instance_id=instance_id,
            log_dir=testbed_log_dir,
            enable_cache=True,
        )

        agent = get_agent(agent_id=self.tree_search_settings.agent_id)
        agent.completion_model = completion_model
        agent.workspace = Workspace(repository=repository, code_index=code_index, runtime=runtime, legacy_workspace=True)

        persist_dir = get_moatless_trajectory_dir(instance_id, self.evaluation.evaluation_name)
        logger.info(f"Persist dir: {persist_dir}")

        if self.tree_search_settings.max_expansions > 1:
            return SearchTree.create(
                message=problem_statement,
                agent=agent,
                repository=repository,
                metadata={"instance_id": instance_id},
                max_iterations=self.tree_search_settings.max_iterations,
                max_expansions=self.tree_search_settings.max_expansions,
                persist_dir=persist_dir,
            )
        else:
            return AgenticLoop.create(
                message=problem_statement,
                run_id=instance_id,
                agent=agent,
                max_iterations=self.tree_search_settings.max_iterations,
                metadata={"instance_id": instance_id},
                persist_dir=persist_dir,
            )

    async def evaluate_nodes(
        self,
        instance_id: str,
        root_node: Node,
        ) -> None:
        """Evaluate all leaf nodes using the testbed."""
        try:
            leaf_nodes = root_node.get_leaf_nodes()

            # Load existing eval results if any
            eval_result = None

            eval_result_path = os.path.join(get_moatless_trajectory_dir(instance_id, self.evaluation.evaluation_name), "eval_result.json")
            if os.path.exists(eval_result_path):
                try:
                    with open(eval_result_path) as f:
                        eval_result = json.load(f)
                        logger.info(f"Loading eval_result from {eval_result_path}")
                        if "node_results" not in eval_result:
                            if len(eval_result) > 0:
                                logger.info(f"Found node results with {eval_result.keys()} on root, fix format")
                                eval_result = {
                                    "node_results": eval_result,
                                    "status": "started",
                                    "start_time": datetime.now(timezone.utc).isoformat(),
                                }
                            else:
                                logger.info("No node_results found")
                                eval_result = None
                        else:
                            logger.info(f"Found evaluated nodes {eval_result['node_results'].keys()}")
                except json.JSONDecodeError:
                    pass

            if not eval_result:
                eval_result = {
                    "node_results": {},
                    "status": "started",
                    "start_time": datetime.now(timezone.utc).isoformat(),
                }

            # Filter out already evaluated nodes
            unevaluated_nodes = [
                node for node in leaf_nodes if str(node.node_id) not in eval_result.get("node_results", {})
            ]

            if not unevaluated_nodes:
                logger.info(f"All {len(leaf_nodes)} nodes for instance {instance_id} have already been evaluated")
                return

            logger.info(
                f"Found {len(leaf_nodes) - len(unevaluated_nodes)} already evaluated nodes, "
                f"will evaluate remaining {len(unevaluated_nodes)} nodes for instance {instance_id}"
            )

            testbed_log_dir = os.path.join(get_moatless_trajectory_dir(instance_id, self.evaluation.evaluation_name), "testbed_logs")
            if not os.path.exists(testbed_log_dir):
                os.makedirs(testbed_log_dir)

            instance = get_moatless_instance(instance_id=instance_id)
            repository = create_repository(instance, repo_base_dir=self.repo_base_dir)
            
            async with TestbedEnvironment(repository=repository, instance_id=instance_id, instance=instance, log_dir=testbed_log_dir) as runtime:
                for i, leaf_node in enumerate(unevaluated_nodes):
                    logger.info(f"Evaluate Node{leaf_node.node_id} {i + 1}/{len(unevaluated_nodes)} for instance {instance_id}")

                    if str(leaf_node.node_id) in eval_result["node_results"]:
                        logger.info(
                            f"Skip Node{leaf_node.node_id} {i + 1}/{len(unevaluated_nodes)} for instance {instance_id} that has already been evaluated"
                        )
                        continue

                    patch = leaf_node.file_context.generate_git_patch(ignore_tests=True)
                    if patch and patch.strip():
                        start_time = time.time()
                        try:
                            result = await runtime.evaluate(patch=patch)
                            if not result:
                                logger.error(f"Error in evaluating patch for {instance_id}")
                                continue

                            eval_result["node_results"][str(leaf_node.node_id)] = result.model_dump()
                            resolved = result.resolved
                            logger.info(
                                f"Evaluated patch for node {leaf_node.node_id} in {time.time() - start_time} seconds (resolved: {result.resolved})"
                            )

                        except Exception as e:
                            logger.exception(f"Error in testbed evaluation for instance {instance_id}")
                            eval_result["error"] = traceback.format_exc()
                        finally:
                            eval_result["duration"] = time.time() - start_time
                            with open(eval_result_path, "w") as f:
                                json.dump(eval_result, f, indent=2)
                    else:
                        logger.info(
                            f"Skip Node{leaf_node.node_id} {i + 1}/{len(unevaluated_nodes)} for instance {instance_id} with no patch."
                        )

            # Emit evaluation completed event
            await self.emit_event("instance_evaluated", {
                "instance_id": instance_id,
                "eval_result": eval_result_path  # Just pass the path instead of the full result
            })

        except Exception as e:
            logger.exception(f"Error evaluating nodes for instance {instance_id}")
            await self.emit_event("instance_error", {
                "instance_id": instance_id,
                "error": str(e)
            })
            raise

    def _clean_error_nodes(self, node: Node, instance_id: str):
        """Clean error nodes if rerun_errors is enabled."""
        last_node = node.get_all_nodes()[-1]

        # Remove error nodes
        if last_node.error or (last_node.action and last_node.action.name == "Error" and last_node.parent):
            last_node.parent.children = [c for c in last_node.parent.children if c.node_id != last_node.node_id]
            logger.info(
                f"Removed error node {last_node.node_id} from parent {last_node.parent.node_id} on instance {instance_id}"
            )

        # Remove nodes with unexecuted actions
        last_node = node.get_all_nodes()[-1]
        if last_node.action and all(not step.observation for step in last_node.action_steps):
            last_node.parent.children = [c for c in last_node.parent.children if c.node_id != last_node.node_id]
            logger.info(
                f"Removed last node {last_node.node_id} from instance {instance_id} because it has no observation"
            )

    def _save_evaluation(self, evaluation: Evaluation):
        """Save evaluation metadata to evaluation.json in a thread-safe manner."""
        eval_path = self.evaluation_dir / "evaluation.json"
        lock_path = self.locks_dir / f"{evaluation.evaluation_name}.lock"
        
        with filelock.FileLock(lock_path):
            with open(eval_path, "w") as f:
                json.dump(evaluation.model_dump(), f, indent=2, default=str)

    def get_evaluation_dir(self) -> str:
        """Get the directory path for an evaluation."""
        return self.evaluation_dir

    async def emit_event(self, event_type: str, data: Any = None):
        """Emit an event to all registered handlers"""
        logger.info(f"Emitting event {event_type}")
        
        self._save_evaluation(self.evaluation)

        event = EvaluationEvent(
            evaluation_name=self.evaluation.evaluation_name,
            event_type=event_type,
            data=data,
        )
        await event_bus.publish(event)

    async def wait_for_completion(self, check_interval: float = 1.0) -> Evaluation:
        """Wait for an evaluation to complete and return the final evaluation object."""
        last_status = None
        while True:
            summary = self.evaluation.get_summary()
            
            # Log status changes
            if summary["status"] != last_status:
                logger.info(f"Evaluation {self.evaluation.evaluation_name} status changed to: {summary['status']}")
                last_status = summary["status"]

            if summary["status"] == EvaluationStatus.COMPLETED:
                logger.info(f"Evaluation {self.evaluation.evaluation_name} completed successfully")
                return self.evaluation
            elif summary["status"] == EvaluationStatus.ERROR:
                logger.error(f"Evaluation {self.evaluation.evaluation_name} failed with error: {summary['error']}")
                raise RuntimeError(f"Evaluation failed: {summary['error']}")

            counts = summary["counts"]
            logger.debug(
                f"Status summary - Completed: {counts['completed']}, Running: {counts['running']}, "
                f"Evaluating: {counts['evaluating']}, Pending: {counts['pending']}, Errors: {counts['errors']}"
            )

            await asyncio.sleep(check_interval)

    def read_node(self, instance_id: str) -> Node:
        """Get the trajectory for an instance."""
        trajectory_dir = get_moatless_trajectory_dir(instance_id)
        trajectory_path = trajectory_dir / "trajectory.json"
        return Node.from_file(trajectory_path)

    async def _evaluate_instance(self, instance: EvaluationInstance):
        """Handle asynchronous evaluation of an instance."""
        try:
            root_node = self.read_node(instance.instance_id)

            await self.evaluate_nodes(
                instance_id=instance.instance_id,
                root_node=root_node
            )

            eval_result_path = os.path.join(
                get_moatless_trajectory_dir(instance.instance_id, self.evaluation.evaluation_name), 
                "eval_result.json"
            )
            eval_result = None
            if os.path.exists(eval_result_path):
                with open(eval_result_path) as f:
                    eval_result = json.load(f)

            instance.benchmark_result = to_result(
                node=root_node,
                eval_report=eval_result,
                instance_id=instance.instance_id,
            )

            instance.resolved = instance.benchmark_result.resolved
            await self._update_instance_state(instance, InstanceStatus.EVALUATED)
            instance.evaluated_at = datetime.now(timezone.utc)
            
            await self.emit_event(
                "instance_evaluated",
                {"instance_id": instance.instance_id, "resolved": instance.resolved}
            )

        except Exception as e:
            logger.exception(f"Error in evaluation: {str(e)}")
            await self._update_instance_state(instance, InstanceStatus.ERROR, str(e))
            await self.emit_event("instance_error",
                                {"instance_id": instance.instance_id, "error": str(e)})

    async def _handle_flow_events(self, trajectory_id: str | None, event: BaseEvent):
        """Handle flow completion and error events."""
        if not trajectory_id:
            return
        
        instance = self.evaluation.get_instance(trajectory_id)
        if not instance:
            return

        if isinstance(event, FlowCompletedEvent):
            logger.info(f"Flow completed for instance {instance.instance_id}")
            instance.completed_at = datetime.now(timezone.utc)
            await self._update_instance_state(instance, InstanceStatus.EVALUATING)
            await self.emit_event("instance_evaluating", {"instance_id": instance.instance_id})
                
        elif isinstance(event, FlowErrorEvent):
            logger.error(f"Flow error for instance {instance.instance_id}: {event.error}")
            instance.completed_at = datetime.now(timezone.utc)
            await self._update_instance_state(instance, InstanceStatus.ERROR, event.error)
            await self.emit_event("instance_error", 
                                {"instance_id": instance.instance_id, "error": event.error})

    def _create_tracked_task(self, coro, instance_id: str) -> asyncio.Task:
        """Create a task and track it for monitoring."""
        task = asyncio.create_task(coro)
        self._tasks.append(task)
        
        def _cleanup(future):
            self._tasks.remove(future)
            if future.exception():
                logger.error(f"Task for instance {instance_id} failed: {future.exception()}")
        
        task.add_done_callback(_cleanup)
        return task

    async def _start_agentic_system(self, agentic_system: AgenticSystem, message: str | None = None) -> str:
        """Start an agentic system and track its execution."""
        run_id = agentic_system.run_id

        async def trajectory_wrapper() -> dict:
            try:
                result = await agentic_system.run(message)
                return result
            finally:
                # Clean up when done
                self._active_runs.pop(run_id, None)

        task = self._create_tracked_task(trajectory_wrapper(), run_id)
        logger.debug(f"Starting run {run_id}")
        self._active_runs[run_id] = (agentic_system, task)
        return run_id

    async def _monitor_status(self):
        """Periodically log status if no changes have occurred."""
        while True:
            current_time = time.time()
            if current_time - self._last_status_log >= self._status_log_interval:
                counts = await self._get_instance_counts()
                self._print_status_with_counts(counts)
                self._last_status_log = current_time
            await asyncio.sleep(self._status_log_interval)

    async def _get_instance_counts(self) -> dict[str, int]:
        """Get current counts of instances in each state."""
        async with self._state_lock:
            counts = {status.value: 0 for status in InstanceStatus}
            for instance in self.evaluation.instances:
                counts[instance.status] += 1
            return counts

    def _print_status_with_counts(self, counts: dict[str, int]):
        """Print current status using provided counts."""
        total = len(self.evaluation.instances)
        
        # Calculate percentages
        percentages = {
            status: (count / total) * 100 if total > 0 else 0 
            for status, count in counts.items()
        }
        
        # Get the longest status name for alignment
        max_status_length = max(len(status) for status in counts.keys())
        
        # Print header
        logger.info(f"\nEvaluation: {self.evaluation.evaluation_name}")
        logger.info("=" * 50)
        
        # Print counts and progress bars
        for status in InstanceStatus:
            status_value = status.value
            count = counts[status_value]
            percentage = percentages[status_value]
            
            # Create a progress bar
            bar_length = 20
            filled_length = int(bar_length * percentage / 100)
            bar = '█' * filled_length + '░' * (bar_length - filled_length)
            
            # Print status line with aligned columns
            logger.info(
                f"{status_value.rjust(max_status_length)}: "
                f"{str(count).rjust(3)} / {total} "
                f"[{bar}] "
                f"{percentage:5.1f}%"
            )
        
        logger.info("=" * 50)
